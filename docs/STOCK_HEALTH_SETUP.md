# Configuration du Use Case Stock Health

## üìã Pr√©requis

### 1. Installer la d√©pendance xlsx

Pour parser les fichiers Excel, vous devez installer le package `xlsx` :

```bash
npm install xlsx
npm install --save-dev @types/xlsx
```

### 2. Configurer Supabase Storage

#### Cr√©er le bucket

1. Allez dans **Supabase Dashboard** ‚Üí **Storage**
2. Cliquez sur **"New bucket"**
3. Configurez le bucket :
   - **Name**: `analysis-files`
   - **Public**: `false` (priv√©)
   - **File size limit**: `10485760` (10MB)
   - **Allowed MIME types**: 
     - `text/csv`
     - `application/vnd.ms-excel`
     - `application/vnd.openxmlformats-officedocument.spreadsheetml.sheet`

#### Configurer les politiques RLS

Ex√©cutez la migration SQL `010_create_storage_bucket.sql` qui configure les politiques RLS pour le storage.

Ou configurez manuellement dans **Storage** ‚Üí **Policies** :

**Policy 1: Upload**
- Name: "Users can upload files to their tenant folder"
- Operation: INSERT
- Target roles: authenticated
- Policy: `bucket_id = 'analysis-files' AND (storage.foldername(name))[1] = (SELECT tenant_id::text FROM profiles WHERE id = auth.uid())`

**Policy 2: Read**
- Name: "Users can read files from their tenant folder"
- Operation: SELECT
- Target roles: authenticated
- Policy: `bucket_id = 'analysis-files' AND (storage.foldername(name))[1] = (SELECT tenant_id::text FROM profiles WHERE id = auth.uid())`

**Policy 3: Delete**
- Name: "Users can delete files from their tenant folder"
- Operation: DELETE
- Target roles: authenticated
- Policy: `bucket_id = 'analysis-files' AND (storage.foldername(name))[1] = (SELECT tenant_id::text FROM profiles WHERE id = auth.uid())`

## üîÑ Flow d'Analyse Stock Health

### 1. Upload de Fichier
- **Route**: `/api/upload`
- **M√©thode**: POST
- **Action**: 
  - Re√ßoit le fichier via FormData
  - Upload vers Supabase Storage (`analysis-files/{tenant_id}/{analysis_id}/{filename}`)
  - Met √† jour l'analyse avec le chemin du fichier

### 2. Phase Mapping
- **Route**: `/api/analyze/map`
- **M√©thode**: POST
- **Action**:
  - T√©l√©charge le fichier depuis Storage
  - Parse le fichier (CSV ou Excel)
  - Appelle Gemini `mapColumns()` pour mapper les colonnes
  - Sauvegarde `original_columns` et `mapped_columns` dans l'analyse
  - D√©clenche automatiquement la phase Cleaning

### 3. Phase Cleaning
- **Route**: `/api/analyze/clean`
- **M√©thode**: POST
- **Action**:
  - T√©l√©charge le fichier depuis Storage
  - Parse le fichier
  - Appelle Gemini `cleanData()` pour nettoyer les donn√©es
  - Ins√®re les donn√©es nettoy√©es dans `stock_entries`
  - Sauvegarde le rapport de nettoyage
  - D√©clenche automatiquement la phase Recommendations

### 4. Phase Recommendations
- **Route**: `/api/analyze/recommend`
- **M√©thode**: POST
- **Action**:
  - R√©cup√®re les `stock_entries` de l'analyse
  - Appelle Gemini `generateRecommendations()` pour g√©n√©rer les recommandations
  - Ins√®re les recommandations dans la DB
  - Met √† jour le statut de l'analyse √† `completed`

### 5. Affichage des R√©sultats
- **Page**: `/stock/[id]`
- **Contenu**:
  - Statistiques du stock (valeur totale, quantit√©, produits uniques)
  - KPIs (Gain potentiel, √âconomie potentielle)
  - Liste des entr√©es de stock
  - Recommandations avec impact estim√©, actions, SKUs concern√©s

## üìù Notes Techniques

- Les phases sont d√©clench√©es de mani√®re s√©quentielle via des appels `fetch()` asynchrones
- Pour la production, consid√©rez utiliser un syst√®me de queue (BullMQ, Inngest, etc.)
- Les fichiers sont stock√©s dans Supabase Storage avec isolation par tenant
- Le parsing CSV est basique (split par virgule) - pour des CSV complexes, utilisez une biblioth√®que d√©di√©e
- Les fichiers Excel n√©cessitent le package `xlsx`

## üêõ D√©pannage

### Erreur "xlsx is not defined"
- Installez le package : `npm install xlsx`

### Erreur "Bucket not found"
- Cr√©ez le bucket `analysis-files` dans Supabase Dashboard ‚Üí Storage

### Erreur "Permission denied" lors de l'upload
- V√©rifiez les politiques RLS du bucket Storage
- V√©rifiez que `SUPABASE_SERVICE_ROLE_KEY` est correctement configur√©

### L'analyse reste en "processing"
- V√©rifiez les logs du serveur pour voir quelle phase a √©chou√©
- V√©rifiez que les routes API sont accessibles
- V√©rifiez que `GOOGLE_GEMINI_API_KEY` est configur√©
